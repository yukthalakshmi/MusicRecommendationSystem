# -*- coding: utf-8 -*-
"""Music Recommendation System - Group 4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19votDds0fAxom8JOoDjXUeTHbD2MC-DT

# **1. Project Summary**

### **Background**

Big companies (Spotify, Apple Music, and so on) have already adapted and developed the new technology trend by releasing their innovative ML-based approaches and services, including the AI solution to recommend songs to users. Through this technology, it is expected to increase user satisfaction and engagement in the music-streaming services as well as make enormous profits against competitors.

### **Objectives & Questions**

We will understand about the music recommendation system, explore all possible machine learning models for building the system, and solve the following business and technical questions.
1. *Which model can be applied to create the music recommendation system?*
2. *How do the models recommend songs to users?*

# **2. Exploratory Data Analysis (EDA)**
This part is exploring the given data set to understand the basic data structure, the meaning of values, and the importance of features for building a music recommendation system. <br>
<br>
The exploratory data analysis will be conducted based on the following process.
1. Import required libraries
2. Upload the data set
3. Understand the data structure
4. Check the samples
5. Investigate missing values
6. Handle outliers
7. Explore the values

### **1) Import required libraries**
"""

# For uploading the file
from google.colab import drive
drive.mount('/content/drive')

# For reading a data frame
import pandas as pd

# For visualization
import seaborn as sns

# Conduct mathematical operations
import numpy as np

"""### **2) Upload the data set**
Upload the data set to the Python environment.
"""

data = pd.read_csv("/content/drive/MyDrive/IST 707 Project/0. Datasets_new/data.csv")

"""### **3) Understand the data structure**
The data set contains audio features of digital songs and information (artist, title, and release year) from Spotify. <br>
There are 170,652 rows (songs) with 19 columns (audio features and song information).
"""

data.info()

"""### **4) Check the samples**
Check the samples to see what the data looks like.

- All audio features have numeric values.
- The `year` and `release_date` columns are the same data.
- The value in the `artist` column is in the brackets, because there may be multiple artists.
"""

data.head(5)

"""### **5) Investigate Missing Values**
Look through the data set to see whether it has missing values or not because the missing values can reduce the quality of outputs from modeling.

There are no missing values in all columns in the data set.
"""

data.isna().sum()

"""### **6) Handle outliers**
Generate a statistical summary and find whether there are outliers in each column in the data set or not. <br>

- The outliers may interfere with making a good performance of an algorithm.

In the below statistical summary, the numeric values in audio features are not on the same scale. We will solve this part for better outputs of the model, deploying normalization.
"""

data.describe()

"""In the box plot of each audio feature after removing the `year` column, which is not the audio feature, there are many outliers in the `duration_ms` and other features."""

data.boxplot(figsize = (20, 5), color = "white")

"""### **7) Explore the values**
Explore the values in the data set to understand what values the data set has.

**1] How many artists are in the data set?** <br>
There are 34,088 (unique) artists in the data set.
"""

len(data['artists'].unique())

"""**2] When were the songs released?** <br>
First, check the basic statistical summary in the `year` column. <br>
Basically, the range of released year of songs in the data set is between **1921** to **2020**.
"""

data['year'].describe()

"""Second, group the values in the `year` column for further analysis in convenience."""

def get_decade(year):
    period_start = int(year/10) * 10
    return str(period_start)

data['year_group'] = data['year'].apply(get_decade)
pd.DataFrame(data.groupby('year_group').size())

"""Then, visualize the result. <br>
Most songs in the data set were released between **1950s and 2010s**.
"""

sns.histplot(data = data, x = 'year_group', binwidth = 3, color = "green")

"""# **3. Modeling**
This part is building music recommendation models with the audio features, based on **Assocation Rules** and **Clustering** (with **Cosine Similarity**)
  <br>
<br>
The data modeling part will be implemented with the following steps. <br>
(Detailed explanations will be provided in each model section.)
1. Data Preprocessing
2. Data Modeling
3. Model Evaluation

## **1) Association Rule**
Association Rule Mining can uncover interesting relationships between items.

**What is Assocation Rule?** <br>
It is one of the popular machine learning algorithms used to find frequent relationships and correlations between variables in large data sets. <br>
<br>
**Is Assocation Rule used in music recommendation systems?** <br>
It usually analyzes the listening patterns of users and finds associations between different songs or artists that are frequently listened to together. Using the audio features, it will find patterns or associations between different songs based on the pre-processed features.

**Advantages**
1. High Accuracy: This algorithm can generate many recommendations with high accuracy that leads to increased user satisfaction and engagement.
2. Better Understand Users: It can discover interesting patterns and associations between itemsets in a large dataset, which are required for allows companies or engineers to understand their users better.

**Limitations**
1. A lack of frequent items: There are may be a lack of frequent item sets that lead to potentially inaccurate recommendations.
2. Significant memory: It requires significant processing power and memory resources if the dataset is large.

**How is the music recommendation system built using Assocation Rule?** <br>
1. Data Preprocessing
> A) Preparation for Modeling <br>
> B) Discretization <br>
> C) One Hot Encoding
2. Data Modeling
> A) `apriori()` <br>
> B) `assocation_rules()` <br>
> C) Reviewing the rules <br>
3. Evaluation
> A) Find the best parameters <br>
> B) Repeat 2-A~C

### **1) Data Preprocessing**

### **1-A) Preparation for Modeling**
Install and upload required libraries.
"""

!pip install apriori

from mlxtend.frequent_patterns import apriori, association_rules

"""Divide the data set into two parts.
1. `data_info`: It consists of song information and is used for searching information of the recommended songs in the recommendation system.
2. `data_audio_features` = It contains the id and song features and is used for modeling the association rules.
"""

data_info = data[['year', 'artists', 'id', 'name', 'release_date']]
data_audio_features = data[['id','valence', 'acousticness', 'danceability', 'duration_ms', 'energy', 'explicit', 'instrumentalness', 'key', 'liveness', 'loudness', 'mode', 'popularity', 'speechiness', 'tempo']]

data_info.head(3)

data_audio_features.head(3)

"""### **1-B) Discretization**
Discretize the audio features, using bins (low, medium, and high) to reduce the number of unique values and make them manageable for AR model to handle.
- low: 0~33%
- medium: 33~66%
- high: 66~100%
"""

df = data_audio_features

df['danceability_bin'] = pd.cut(df['danceability'], bins=[0, np.percentile(df['danceability'], 0.33), np.percentile(df['danceability'], 0.66), 1], labels=['low', 'medium', 'high'])
df['energy_bin'] = pd.cut(df['energy'], bins=[0, np.percentile(df['energy'], 0.33), np.percentile(df['energy'], 0.66), 1], labels=['low', 'medium', 'high'])
df['loudness_bin'] = pd.cut(df['loudness'], bins=[-60, np.percentile(df['loudness'], 0.33), np.percentile(df['loudness'], 0.66), 0], labels=['low', 'medium', 'high'])
df['speechiness_bin'] = pd.cut(df['speechiness'], bins=[0, np.percentile(df['speechiness'], 0.33), np.percentile(df['speechiness'], 0.66), 1], labels=['low', 'medium', 'high'])
df['acousticness_bin'] = pd.cut(df['acousticness'], bins=[0, np.percentile(df['acousticness'], 0.33), np.percentile(df['acousticness'], 0.66), 1], labels=['low', 'medium', 'high'])
df['instrumentalness_bin'] = pd.cut(df['instrumentalness'], bins=[0,0.33,0.66, 1], labels=['low', 'medium', 'high'])
df['liveness_bin'] = pd.cut(df['liveness'], bins=[0, np.percentile(df['liveness'], 0.33), np.percentile(df['liveness'], 0.66), 1], labels=['low', 'medium', 'high'])
df['valence_bin'] = pd.cut(df['valence'], bins=[0, np.percentile(df['valence'], 0.33), np.percentile(df['valence'], 0.66), 1], labels=['low', 'medium', 'high'])
df['tempo_bin'] = pd.cut(df['tempo'], bins=[0, np.percentile(df['tempo'], 0.33), np.percentile(df['tempo'], 0.66), 220], labels=['slow', 'medium', 'fast'])

"""### **1-C) One Hot Encoding**
Convert the discretized data into a binary format that is compatible with the AR Model
"""

one_hot = pd.get_dummies(df[['danceability_bin', 'energy_bin', 'loudness_bin', 'speechiness_bin', 'acousticness_bin', 'instrumentalness_bin', 'liveness_bin', 'valence_bin', 'tempo_bin']])

"""### **2) Data Modeling**
Run the apriori algorithm with the pre-processed audio features to find the best rules for recommendation systems.

### **2-A) `apriori()`**

First, create the frequent itemsets using `apriori()`.
"""

frequent_itemsets = apriori(one_hot, min_support = 0.05, use_colnames=True)

"""### **2-B) `assocation_rules()`**

Then, define the association rules based on the frequent itemsets. <br>
In this case, set "`support`" as the metric to find the most frequent itemset with which appears in a dataset.
"""

rules = association_rules(frequent_itemsets, metric="support", min_threshold=0.5)

"""### **2-C) Reviewing the rules**

As see the first few rules, most items that describes activeness are associated with other active audio features. (Energy, Danceability, Loudness and so on.)
"""

rules[['antecedents', 'support', 'confidence', 'lift', 'consequents']]

"""### **3) Evaluation**
Based on the above baseline, find the best parameter to show the best assocations (rules) in the audio feature data set.
"""

output = rules[['antecedents', 'confidence', 'support', 'lift', 'consequents']]

"""First, check the rules with the descending order of `lift`."""

output.nlargest(n = 5, columns = 'lift')

"""Then, check the rules with a different order (`confidence` by descending)."""

output.nlargest(n = 5, columns = 'confidence')

"""Next, check the rules with the other order (`support` by descending)."""

output.nlargest(n = 5, columns = 'support')

"""Typically, the goal is to identify strong association between features that appear frequently. <br>
Therefore, sort the rules with the three measures (confidence, lift, and support) by descending order and print the best five rules.

The sequence of measures (confidence, lift, and support) is important in the above code.
1. Confidence and lift indicate which item sets have the strongest association.
2. Support describes which item sets appear frequently in the data set.
3. Overall, find the strongest rules (associations) using confidence and lift, and then select the rules which appear frequently in the set.
"""

output.nlargest(n = 5, columns = ['confidence', 'lift', 'support'])

"""Based on the findings from the above model, we will develop a new recommendation sysetem in Part 4.

## **2) Clustering**
Clustering makes groups with similar songs and creates the recommended playlist based on the user's preference (or request).

**What is clustering?** <br>
It involves grouping similar data points into clusters based on their distance or similarity. <br>
<br>
**Is clustering used in music recommendation systems?** <br>
It can be used in music recommendation systems by clustering songs based on the audio features and identifying groups of songs that share similar musical characteristics and recommend new songs to users based on their preferences. <br>
<br>
**How can be cosine similarity used in the clustering with the music recommendation system?** <Br>
It is used to calculate the pairwise cosine similarity between songs based on their audio features and use it as a distance measure for clustering.

**Advantages**
1. Simple and easy to implement: Using only clustering for music recommendation is a simple and easy-to-implement approach that does not require complex algorithms or data processing.
2. Can work well for some users: Clustering can be effective for users who have a specific music preference or a limited listening history, as it groups similar songs together and recommends songs within the same cluster.

**Limitations**
1. Limited personalization: Using only clustering may not provide a personalized recommendation for the user, as it only recommends songs based on their similairty to other songs in the same cluster.
2. May not capture diverse music tastes: Clustering may not be effective for users who have diverse music tastes or are open to exploring new genres and artists, as it tends to recommend songs that are similar to what the user already likes.

**How is the music recommendation system built using clustering?** <br>
1. Data Preprocessing
> A) Normalization <br>
> B) Dimensionality Reduction
2. Data Modeling
> A) K-means Clustering <br>
> B) DBSCAN Clustering <br>
> C) Hierarchical (ward linkage) and Agglomerative Clustering <br>
> D) Spectral Clustering
3. Evaluation
> A) Avg. Silhouette Scores <br>
> B) Calinski-Harabasz Scores

### **1) Data Preprocessing**

### **1-A) Normalization**
Make all audio features (numeric values) on a common scale with equal importance, removing outliers from some audio features.
"""

data2 = data

songs = data2.drop(['id', 'name', 'release_date', 'artists', 'mode', 'explicit', 'duration_ms', 'acousticness', 'year', 'popularity', 'danceability', 'loudness', 'tempo'], axis = 1)

"""Data Normalization"""

from sklearn.preprocessing import MinMaxScaler, LabelEncoder
songs_normalized = songs.copy()
scaler = MinMaxScaler()

for i in songs.columns:
  songs_normalized[i] = scaler.fit_transform(songs_normalized[[i]])

"""Check the results using visualization"""

import pandas as pd
import matplotlib.pyplot as plt

# Create a figure with subplots for each feature
fig, axs = plt.subplots(nrows=4, ncols=4, figsize=(20, 20))

# Loop through each feature and draw a boxplot on its respective subplot
for i, feature in enumerate(songs_normalized.columns):
    row = i // 4
    col = i % 4
    axs[row, col].boxplot(songs_normalized[feature])
    axs[row, col].set_title(feature)

plt.tight_layout()
plt.show()

X = songs.sample(frac = 0.1, random_state=8).values

"""### **1-B) Dimensionality Reduction**
Reduce the computational complexity of clustering algorithms that contains normalized audio feature values and improve the accuracy, using **PCA** and **t-SNE** techniques.

- **PCA (Principal Component Analysis)**: A linear dimensionality reduction technique that identifies the most important variables and captures the maximum amount of variance in the data.
- **t-SNE (t-Distributed Stochastic Neighbor Embedding)**: A non-linear dimensionality reduction technique that preserves the local structure of the data and identifies non-linear relationships between data points.

**PCA**
"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Load the dataset and extract features
# X = extract_features(data)

# Normalize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Instantiate the PCA object with desired number of components
n_components = 2
pca = PCA(n_components=n_components)

# Fit and transform the data
X_pca = pca.fit_transform(X_scaled)

# View the explained variance ratio for each principal component
print(pca.explained_variance_ratio_)

"""**The explained variance ration [0.26611292 0.18236745] indicates the following findings.**
- The first principal component explains 26.6% of the total variance in the data.
- The second principal component explains 18.2% of the total variance.
- Combining these two principal components, these two principal components explain 44.8% of the total variance in the data. (Jolliffe, 2011)
"""

plt.scatter([i[0] for i in X_pca], [i[1] for i in X_pca])

"""**t-SNE** <br>
There is no general way to evaluate the quality of t-SNE embeddings, we will evalaute the performance in the clustering model part. (Maaten and Hinton, 2008)
"""

from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler

# Normalize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Instantiate the t-SNE object with desired number of components and perplexity
n_components = 2
perplexity = 10
tsne = TSNE(n_components=n_components, perplexity=perplexity)

# Fit and transform the data
X_tsne = tsne.fit_transform(X_scaled)

# Plot the t-SNE visualization
import matplotlib.pyplot as plt
plt.scatter(X_tsne[:, 0], X_tsne[:, 1])
plt.xlabel('t-SNE component 1')
plt.ylabel('t-SNE component 2')
plt.show()

"""### **2) Data Modeling**

### **Finding the best k value (the number of Clustering)**
Before clustering, find the best k value (the number of clustering) using Elbow Method and Silhouette Score)

**Elbow Method**<br>
The best K value = 3
"""

from sklearn.cluster import KMeans

# calculate WCSS for different number of clusters
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

# plot the elbow curve
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')

# show the best k value
best_k = 3
plt.axvline(x=best_k, linestyle='--', color='red')
plt.text(best_k + 0.1, max(wcss)/2, f'Best K: {best_k}')

plt.show()

"""**Silhouette Score** <br>
The best K value = 10

"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# calculate Silhouette score for different number of clusters
silhouette_scores = []
for i in range(2, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(X)
    labels = kmeans.labels_
    silhouette_scores.append(silhouette_score(X, labels))

# plot the Silhouette score
plt.plot(range(2, 11), silhouette_scores)
plt.title('Silhouette Score')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette score')

# show the best k value
best_k = silhouette_scores.index(max(silhouette_scores)) + 2
plt.axvline(x=best_k, linestyle='--', color='red')

plt.show()

"""### **2-A) K-Mean Clustering**
K-Mean Clustering partitions a dataset into k clusters.

**Apply the K-Mean clustering model to the dataset**
"""

# K means clustering

from sklearn.cluster import KMeans

# Initialize k-means object
kmeans = KMeans(n_clusters=5, random_state=42)

# Fit k-means model to data
kmeans.fit(X)

# Predict cluster labels for new data points
cluster_labels = kmeans.predict(X)

# Extract centroids of each cluster
centroids = kmeans.cluster_centers_
print(centroids)

"""**Plot the clusters**

1) t-SNE Perspective
"""

# Plot the clusters
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=cluster_labels)
plt.title("K-means Clustering")
plt.show()

"""2) PCA Perspective"""

# Plot the clusters
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels)
plt.title("K-means Clustering")
plt.show()

"""### **2-B) DBSCAN Clustering**
It uses the closeness to each data points in a feature space and separating regions of low density.

**Apply the DBSCAN clustering model to the dataset**
"""

from sklearn.cluster import DBSCAN
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

# Create a DBSCAN object
dbscan = DBSCAN(eps=0.5, min_samples=5)

# Fit and predict
y_pred_dbscan = dbscan.fit_predict(X)

"""t-SNE Perspective"""

# Plot the clusters
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_pred_dbscan)
plt.title("DBSCAN Clustering")
plt.show()

"""PCA Perspective"""

# Plot the clusters
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred_dbscan)
plt.title("DBSCAN Clustering")
plt.show()

"""### **2-C) Hierarchical with ward linkage**
It is a bottom-up approach that creates a tree-like structure of clusters.

**Apply the Hierarchical with ward linkage clustering model to the dataset**

t-SNE Persepctive
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage

# Compute the linkage matrix
Z = linkage(X_tsne, 'ward')

# Plot the dendrogram
fig = plt.figure(figsize=(25, 10))
dn = dendrogram(Z)
plt.show()

"""PCA Perspective"""

# Compute the linkage matrix
Z1 = linkage(X_pca, 'ward')

# Plot the dendrogram
fig = plt.figure(figsize=(25, 10))
dn = dendrogram(Z1)
plt.show()

"""**Run the agglomerative clustering**

"""

from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt

# Create an agglomerative clustering object
agg_clustering = AgglomerativeClustering(n_clusters=5, linkage='ward')

# Fit and predict
y_pred_ward = agg_clustering.fit_predict(X)

"""t-SNE Perspective"""

# Plot the clusters
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_pred_ward)
plt.title("Agglomerative Clustering")
plt.show()

"""PCA Perspective"""

# Plot the clusters
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred_ward)
plt.title("Agglomerative Clustering")
plt.show()

"""### **2-D) Spectral Clustering**
It uses eigenvalues and eigenvectors of a similar matrix to clusters.

**Apply the Spectral clustering model to the dataset**
"""

from sklearn.cluster import SpectralClustering
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

# Create a spectral clustering object
sc = SpectralClustering(n_clusters=5, affinity='nearest_neighbors', n_neighbors=10)

# Fit and predict
y_pred_spec = sc.fit_predict(X)

"""t-SNE Perspective"""

# Plot the clusters
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_pred_spec)
plt.title("Spectral Clustering")
plt.show()

"""PCA Perspective"""

# Plot the clusters
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred_spec)
plt.title("Spectral Clustering")
plt.show()

"""### **3) Evaluation**
Evaluate the above cluster models, computing Avg. Silhouette and Calinski-Harabasz Scores.

**What is Avg. Silhouette score?**<br>
It measures how well each data point fits into its assigned cluster. <br>
<br>
**What is Calinski-Harabasz score?** <br>
It measures the ratio of between-cluster variance to the within cluster variance.
<br>
<br>
**What is the result?** <br>
Based on the results, **K-Means** will be the best model to build clustering model. <br>
- **Avg. Silhouette Score (0.503)**: The data point in k-mean clusters is very well mached to its own cluster, but not to neighboring clusters.
- **Calinski-Harabasz Score (74711.500)**: It shows the better clustering results.

### **3-A) Avg. Silhouette Scores**
"""

from sklearn.metrics import silhouette_score

# K-mean
silhouette_avg_k = silhouette_score(X, cluster_labels)
print("The average silhouette score of K-Mean is :", silhouette_avg_k)

# DBSCAN
silhouette_avg_d = silhouette_score(X, y_pred_dbscan)
print("The average silhouette score of DBSCAN is :", silhouette_avg_d)

# Hierarchical with ward linkage
silhouette_avg_h = silhouette_score(X, y_pred_ward)
print("The average silhouette score of Hierarchical with ward linkage is :", silhouette_avg_h)

# Spectral
silhouette_avg_s = silhouette_score(X, y_pred_spec)
print("The average silhouette score of Spectral is :", silhouette_avg_s)

"""### **3-B) Calinski-Harabasz Scores**"""

from sklearn.metrics import calinski_harabasz_score

# K-mean
ch_score_k = calinski_harabasz_score(X, cluster_labels)
print("The average calinski-harabasz scores of K-Mean is :", ch_score_k)

# DBSCAN
ch_score_d = calinski_harabasz_score(X, y_pred_dbscan)
print("The average calinski-harabasz scores of DBSCAN is :", ch_score_d)

# Hierarchical with ward linkage
ch_score_h = calinski_harabasz_score(X, y_pred_ward)
print("The average calinski-harabasz scores of Hierarchical with ward linkage is :", ch_score_h)

# Spectral
ch_score_s = calinski_harabasz_score(X, y_pred_spec)
print("The average calinski-harabasz scores of Spectral is :", ch_score_s)

"""# **4. Preliminary Analysis**
This part is defining the preliminary conclusion from the above research, implementing application (developing recommendation systems), and make suggestions and improvements for further studies. <br>
<br>
The preliminary analysis part will be followed with the below steps.
1. Findings
2. Applications
3. Improvements (Suggestions for Further Studies)

## **1) Findings**

**Association Rule** <br>
- Metric = Support (Min = 0.05, Threshold = 0.5)
- Most active* digital songs contains instrumental-based features in the frequent item sets.
> Active: Danceability, energy, valence**, loudness <br>
> Valence: It shows the sentiment vibe of a song. The high valence means the song sounds more positive. (Santos, 2017)

**Clustering**
- Best Model: K-Means Clustering
- It shows the highest performance of matching data points in the own clusters and better clustering results.

## **2) Applications**

Input song title, artist name, or released year into the system and evaluate which model (AR or Clustering) is the best and whether it recommends s﻿ongs that a user feels satisfied with or not.

### **Assocation Rule**
"""

def recommend_ar(artist_name="", song_name=None, year=None):
    # Only artist name
    input_row = pd.Series()
    if artist_name != "" and song_name == "" and year is None:
        artist_one_hot = pd.get_dummies(data[data['artists'] == artist_name]['artists']).max()
        input_row = artist_one_hot
    # Only released year
    elif artist_name == "" and song_name == "" and year is not None:
        year_one_hot = pd.get_dummies(data[data['year'] == year]['year']).max()
        input_row = year_one_hot
    # Only Song name
    elif artist_name == "" and song_name != "" and year is None:
        song_one_hot = pd.get_dummies(data[data['name'] == song_name]['name']).max()
        input_row = song_one_hot
    # Year and artist name
    elif artist_name != "" and song_name != "" and year is not None:
        artist_one_hot = pd.get_dummies(data[data['artists'] == artist_name]['artists']).max()
        year_one_hot = pd.get_dummies(data[data['year'] == year]['year']).max()
        input_row = pd.concat([artist_one_hot, year_one_hot])

    # Apply association rules to generate recommendations
    frequent_itemsets = apriori(one_hot, min_support=0.05, use_colnames=True)
    recommendations = association_rules(frequent_itemsets, metric="support", min_threshold=0.5)
    recommendations = recommendations[recommendations['antecedents'].apply(lambda x: set(input_row) <= x)]

    # Sort recommendations by support, confidence, and lift, and return the top 10
    recommendations = recommendations.sort_values(by=['confidence', 'lift', 'support'], ascending=False).head(10)
    recommended_songs_index = recommendations.index
    recommended_songs = data.iloc[recommended_songs_index]

    return recommended_songs[['artists', 'name', 'year']]

artist_name = input("Artist Name (Default): ")
song_name = input("Song Title (Optional): ")
year = input("Year (Optional): ")

recommendations = recommend_ar(artist_name, song_name, year)
pd.DataFrame(recommendations)

"""### **Clustering**"""

from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
def recommend_c(data, artist_name="", song_name=None, year=None):
    # Clustering
    df = data
    # Normalization
    songs = df.drop(['id', 'name', 'release_date', 'artists', 'mode', 'explicit', 'duration_ms', 'acousticness', 'year', 'popularity', 'danceability', 'loudness', 'tempo'], axis = 1)
    #songs = songs[['year_group']].apply(pd.to_numeric)
    songs['year_group'], mapping = songs['year_group'].factorize()
    songs_normalized = songs.copy()
    scaler = MinMaxScaler()

    for i in songs.columns:
        songs_normalized[i] = scaler.fit_transform(songs_normalized[[i]])

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(songs_normalized)

    # PCA
    n_components = songs_normalized.shape[1]
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X_scaled)

    # Initialize k-means object
    n_clusters = min(5, X_pca.shape[0])
    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)
    kmeans.fit(X_pca)
    cluster_labels = kmeans.predict(X_pca)
    centroids = kmeans.cluster_centers_

    # Get the features used in clustering
    features = ['valence', 'energy', 'instrumentalness', 'key', 'liveness', 'speechiness', 'decade']
    cluster_features = [f for f in features if f in data.columns]

    # input data
    input_row = pd.Series()
    if artist_name != "" and song_name == "":
        df = data[data['artists']=="[\'" + artist_name +"\']"]
        df['year_group'], mapping = df['year_group'].factorize()
        input_row = df.drop(['id', 'name', 'release_date', 'artists', 'mode', 'explicit', 'duration_ms', 'acousticness', 'year', 'popularity', 'danceability', 'loudness', 'tempo'], axis = 1).fillna(0).mean()
    # Only released year
    elif artist_name == "" and song_name == "" and year is not None:
        df = data[data['year']==year]
        df['year_group'], mapping = df['year_group'].factorize()
        input_row = df.drop(['id', 'name', 'release_date', 'artists', 'mode', 'explicit', 'duration_ms', 'acousticness', 'year', 'popularity', 'danceability', 'loudness', 'tempo'], axis = 1).fillna(0).mean()
    # Only Song name
    elif artist_name == "" and song_name != "" and year is None:
        df = data[data['name']==song_name]
        df['year_group'], mapping = df['year_group'].factorize()
        input_row = df.drop(['id', 'name', 'release_date', 'artists', 'mode', 'explicit', 'duration_ms', 'acousticness', 'year', 'popularity', 'danceability', 'loudness', 'tempo'], axis = 1).fillna(0).mean()
    # Year and artist name
    elif artist_name != "" and song_name != "" and year is not None:
        df = data[data['name']==song_name & data['year']==year & data['artists']==artist_name]
        df['year_group'], mapping = df['year_group'].factorize()
        input_row = df.drop(['id', 'name', 'release_date', 'artists', 'mode', 'explicit', 'duration_ms', 'acousticness', 'year', 'popularity', 'danceability', 'loudness', 'tempo'], axis = 1).fillna(0).mean()

    # Normalize input row
    input_row = scaler.transform([input_row])

    # Compute cosine similarity between the input row and each cluster centroid
    similarities = cosine_similarity(input_row, centroids)

    # Get the top 10 songs with the highest cosine similarity to the input row based on the popularity
    indices = np.argsort(similarities)[0][::-1][:10]
    recommended_songs_index = [i for i in range(len(cluster_labels)) if cluster_labels[i] in indices]
    recommended_songs = data.iloc[recommended_songs_index].sort_values(by="popularity", ascending = False).head(10)

    return recommended_songs[['artists', 'name', 'year']]

artist_name = input("Artist Name (Default): ")
song_name = input("Song Title (Optional): ")
year = input("Year (Optional): ")

recommendations = recommend_c(data, artist_name, song_name, year)
recommendations

"""Based on the result, we would say the **clustering model** is more manageable and acceptable for building music recommendation systems.

## **3) Improvements (Suggestions for Further Studies)**

To evaluate and improve the music recommendation system, we also need user data sets (personal information or listening history) to conduct the reinforcement learning for ranking the recommended songs and presenting the best.

# **5. References**

Jolliffe, I. T. (2011). Principal component analysis. Springer.

Maaten, L. V. D., & Hinton, G. (2008). Visualizing data using t-SNE. Journal of Machine Learning Research, 9(Nov), 2579-2605.

Scikit-learn. (n.d.). Clustering performance evaluation. Retrieved September 28, 2021, from https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation

Santos, J. D. D. (2017, May 31). Is my Spotify music boring? an analysis involving music, data, and machine learning. Medium. Retrieved April 2, 2023, from https://towardsdatascience.com/is-my-spotify-music-boring-an-analysis-involving-music-data-and-machine-learning-47550ae931de

Tan, P. N., Steinbach, M., & Kumar, V. (2006). Introduction to data mining. Addison-Wesley Longman Publishing Co., Inc.
"""